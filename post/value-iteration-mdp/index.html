<!doctype html><html lang=en itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>Value Iteration Algorithm for a Discrete Markov Decision Process - Prashant Meckoni</title><meta name=description content="
The Value Iteration algorithm also known as the Backward Induction algorithm is one of the simplest dynamic programming algorithm for determining the best policy for a markov decision process. "><meta name=author content="Prashant Meckoni"><script type=application/ld+json>{"@context":"http://schema.org","@type":"WebSite","name":"Prashant Meckoni","url":"https:\/\/pmeckoni.github.io"}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"Organization","name":"","url":"https:\/\/pmeckoni.github.io"}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"https:\/\/pmeckoni.github.io","name":"home"}},{"@type":"ListItem","position":3,"item":{"@id":"https:\/\/pmeckoni.github.io\/post\/value-iteration-mdp\/","name":"Value iteration algorithm for a discrete markov decision process"}}]}</script><script type=application/ld+json>{"@context":"http://schema.org","@type":"Article","author":{"name":"Prashant Meckoni"},"headline":"Value Iteration Algorithm for a Discrete Markov Decision Process","description":" The Value Iteration algorithm also known as the Backward Induction algorithm is one of the simplest dynamic programming algorithm for determining the best policy for a markov decision process. ","inLanguage":"en","wordCount":636,"datePublished":"2019-03-23T00:00:00","dateModified":"2019-03-23T00:00:00","image":"https:\/\/pmeckoni.github.io\/img\/prashant_square.jpg","keywords":[""],"mainEntityOfPage":"https:\/\/pmeckoni.github.io\/post\/value-iteration-mdp\/","publisher":{"@type":"Organization","name":"https:\/\/pmeckoni.github.io","logo":{"@type":"ImageObject","url":"https:\/\/pmeckoni.github.io\/img\/prashant_square.jpg","height":60,"width":60}}}</script><meta property="og:title" content="Value Iteration Algorithm for a Discrete Markov Decision Process"><meta property="og:description" content="
The Value Iteration algorithm also known as the Backward Induction algorithm is one of the simplest dynamic programming algorithm for determining the best policy for a markov decision process. "><meta property="og:image" content="https://pmeckoni.github.io/img/prashant_square.jpg"><meta property="og:url" content="https://pmeckoni.github.io/post/value-iteration-mdp/"><meta property="og:type" content="website"><meta property="og:site_name" content="Prashant Meckoni"><meta name=twitter:title content="Value Iteration Algorithm for a Discrete Markov Decision Process"><meta name=twitter:description content="
The Value Iteration algorithm also known as the Backward Induction algorithm is one of the simplest dynamic programming algorithm for determining the best policy for a markov decision process. "><meta name=twitter:image content="https://pmeckoni.github.io/img/prashant_square.jpg"><meta name=twitter:card content="summary"><link href=https://pmeckoni.github.io/img/favicon.ico rel=icon type=image/x-icon><meta name=generator content="Hugo 0.83.1"><link rel=alternate href=https://pmeckoni.github.io/index.xml type=application/rss+xml title="Prashant Meckoni"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css integrity=sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.5.0/css/all.css integrity=sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU crossorigin=anonymous><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css integrity=sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u crossorigin=anonymous><link rel=stylesheet href=https://pmeckoni.github.io/css/main.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic"><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800"><link rel=stylesheet href=https://pmeckoni.github.io/css/highlight.min.css><link rel=stylesheet href=https://pmeckoni.github.io/css/codeblock.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css integrity=sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css integrity=sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R crossorigin=anonymous></head><body><nav class="navbar navbar-default navbar-fixed-top navbar-custom"><div class=container-fluid><div class=navbar-header><button type=button class=navbar-toggle data-toggle=collapse data-target=#main-navbar>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span><span class=icon-bar></span><span class=icon-bar></span></button>
<a class=navbar-brand href=https://pmeckoni.github.io>Prashant Meckoni</a></div><div class="collapse navbar-collapse" id=main-navbar><ul class="nav navbar-nav navbar-right"><li><a title=Articles href=/post/>Articles</a></li></ul></div><div class=avatar-container><div class=avatar-img-border><a title="Prashant Meckoni" href=https://pmeckoni.github.io><img class=avatar-img src=https://pmeckoni.github.io/img/prashant_square.jpg alt="Prashant Meckoni"></a></div></div></div></nav><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header class=header-section><div class="intro-header no-img"><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div class=post-heading><h1>Value Iteration Algorithm for a Discrete Markov Decision Process</h1><span class=post-meta><i class="fas fa-calendar"></i>&nbsp;Posted on Mar 23, 2019</span></div></div></div></div></div></header><div class=container role=main><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><article role=main class=blog-post><p>The Value Iteration algorithm also known as the Backward Induction algorithm is one of the simplest dynamic programming algorithm for determining the best policy for a markov decision process.</p><div id=outline-container-headline-1 class=outline-2><h2 id=headline-1>Finite Horizon</h2><div id=outline-text-headline-1 class=outline-text-2><p>Consider a Discrete Time Markov Decision Process with a finite horizon with deterministic policy.
We can characterize this Markov Decision Process with as \( (T, S, A, P, R) \) where
\begin{align*}
t &= 1 \ldots T \text{ represents the epochs}<br>A &\text{ is the set of Actions such that } a_t ∈ A<br>S &\text{ is the set of States such that } s_t ∈ S<br>P &\text{ is the Probability Transition Matrix such that } p(s<sub>t+1</sub> \mid s_t, a_t) = P_{(s_t, a_t),s<sub>t+1</sub>}<br>R &\text{ is the Reward Matrix such that reward } r(s_t, a_t) = R<sub>s_t,a_t</sub>
\end{align*}</p><p>Additional notations are:</p><ol><li><p>The policy \( a_t = \pi(s_t) \) is deterministic for each state \( s_t \) and the set of all possible policies is \( \Pi \). \( \pi \in \Pi \). We denote \( \pi \) as the vector indexed by the state \( s_t \).</p></li><li><p>The discount factor is \( 0 \le \gamma \le 1 \).</p></li><li><p>The utility defined here as <strong>expected total discounted reward</strong> of a policy \( \pi \) is \[u_t^{\pi} (s_t) = r(s_t, a_t) + \gamma \sum_{s_{t+1} \in S} p(s_{t+1} \mid s_t, a_t) u_{t+1}^{\pi} (s_{t+1}) \]</p></li><li><p>The best (maximum) utility can be given by \[ u_t^*(s_t) = \max_{\pi \in \Pi} u_t^{\pi}(s_t) \]</p></li><li><p>The residual reward (scrap value) at the end of the horizon \( (T) \) is \( r(s_T) \)</p></li></ol><p>The Bellman equations give us
\[ u_t^*(s_t) = \max_{a_t \in A} \Big[ r(s_t, a_t) + \gamma \sum_{s_{t+1} \in S} p(s_{t+1} \mid s_t, a_t) u_{t+1}^{*} (s_{t+1}) \Big] \]</p><p>The Bellman principle of optimality is a very simple yet powerful statement:</p><blockquote><p>Principle of Optimality:
An optimal policy has the property that whatever the initial state and initial decisions are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decisions.</p></blockquote><p>In other words, the Bellman optimality equation tells us that to get the maximum total discounted reward, we must recursively choose an action at each step that provides the maximum of the sum of reward due to action at this time epoch and the discounted maximum of the total discounted reward for the next time epoch.</p><p>We can solve this by using the <strong>Value Iteration Algorithm</strong> which is also known as the <strong>Backward Induction Algorithm</strong></p><p>Algorithm is as follows:</p><p>Step 0. Set \( t = T,\quad u_T^{*}(s_T) = r(s_T) \quad \forall s_T \in S \).</p><p>Step 1. for \( t = T-1 \ldots 0 \) do:</p><p>Step 1.1 Calculate \( u_t^{*}(s_t) \) as \[ u_t^{*}(s_t) = \max_{a_t \in A} \Big[ r(s_t, a_t) + \gamma \sum_{s_{t+1} \in S} p(s_{t+1} \mid s_t, a_t) u_{t+1}^{*} (s_{t+1}) \Big] \quad \forall s_t \]</p><p>Step 1.2 Determine the best policy \( \pi(s_t) \) using \[ \pi^{*}(s_t) = \text{arg} \max_{a_t \in A} \Big[ r(s_t, a_t) + \gamma \sum_{s_{t+1} \in S} p(s_{t+1} \mid s_t, a_t) u_{t+1}^{*} (s_{t+1}) \Big] \quad \forall s_t \]</p></div></div><div id=outline-container-headline-2 class=outline-2><h2 id=headline-2>Infinite Horizon:</h2><div id=outline-text-headline-2 class=outline-text-2><p>The basic premise for an infinite horizon markov decision process with the utility as the expected total discounted is the convergence of the geometric series.</p><p>\[ \sum_{n=0 }^{\infty} x^n = \frac {1}{1 - x} \quad \forall \mathopen| x \mathclose| &lt; 1 \]</p><p>So for a discount factor of \(0 \le \gamma &lt; 1\) and any reward \(r &lt; \infty \), we will get</p><p>\[ \sum_{n=0}^{\infty} \gamma^n r = \frac{1}{1-\gamma} r &lt; \infty\].</p><p>The use of the discount factor will converge the total expected reward. As we deal with computing technology with limited precision, we will eventually see that the improvement in utility falls below the precision that the computer can support. This can become a stopping criteria or we can choose an the desired precision \( e \).</p></div></div></article><ul class="pager blog-pager"><li class=previous><a href=https://pmeckoni.github.io/post/keeping-warm-new-england-winter/ data-toggle=tooltip data-placement=top title="Keeping warm during winter in New England">&larr; Previous Post</a></li><li class=next><a href=https://pmeckoni.github.io/post/doctoral-colloquium-informs-2019/ data-toggle=tooltip data-placement=top title="Doctoral Student Colloquium Notes">Next Post &rarr;</a></li></ul></div></div></div><footer><div class=container><div class=row><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><ul class="list-inline text-center footer-links"><li><a href=mailto:pmeckoni@gmail.com title="Email me"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i></span></a></li><li><a href title=RSS><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></li></ul><p class="credits copyright text-muted"><a href=pmeckoni.github.io>Prashant Meckoni</a>
&nbsp;&bull;&nbsp;&copy;
2021
&nbsp;&bull;&nbsp;
<a href=https://pmeckoni.github.io>Prashant Meckoni</a></p><p class="credits theme-by text-muted"><a href=https://gohugo.io>Hugo v0.83.1</a> powered &nbsp;&bull;&nbsp; Theme <a href=https://github.com/halogenica/beautifulhugo>Beautiful Hugo</a> adapted from <a href=https://deanattali.com/beautiful-jekyll/>Beautiful Jekyll</a></p></div></div></div></footer><script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js integrity=sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js integrity=sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe crossorigin=anonymous></script><script src=https://code.jquery.com/jquery-1.12.4.min.js integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin=anonymous></script><script src=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js integrity=sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa crossorigin=anonymous></script><script src=https://pmeckoni.github.io/js/main.js></script><script src=https://pmeckoni.github.io/js/highlight.min.js></script><script>hljs.initHighlightingOnLoad()</script><script>$(document).ready(function(){$("pre.chroma").css("padding","0")})</script><script>renderMathInElement(document.body)</script><script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js integrity=sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js integrity=sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q crossorigin=anonymous></script><script src=https://pmeckoni.github.io/js/load-photoswipe.js></script></body></html>